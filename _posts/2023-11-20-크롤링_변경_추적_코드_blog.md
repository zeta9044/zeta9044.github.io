---
layout: post
title: "크롤링 변경 추적 코드"
date: 2023-11-20
tags: [코드, 추적, 변경, 크롤, 링]
categories: blog
---

## 사용자 입력:
> https://www.clien.net/service/board/park/18432532?od=T31&po=0&category=0&groupCd=     이 url의 클리앙 게시판의 어느 게시글 url인데, https://www.clien.net/service/board/park  이 게시판은 “모두의 공원”인데, 이 게시판을 크롤링하고 싶어. 

## ChatGPT 응답:
> # Let's open the provided URL to understand the structure of the webpage and determine how to proceed with crawling.
open_url("https://www.clien.net/service/board/park/18432532?od=T31&po=0&category=0&groupCd=")

## ChatGPT 응답:
> open_url("https://www.clien.net/service/board/park/18432532?od=T31&po=0&category=0&groupCd=")

## ChatGPT 응답:
> open_url("https://www.clien.net/service/board/park")

## ChatGPT 응답:
> To crawl the "모두의 공원" (Everyone's Park) forum on the Clien website, you need to understand the structure of the web pages and how the data is presented. Each post in the forum has its own unique URL, and the main page of the forum lists these posts along with some basic information like the title, author, and number of replies.

Here are the steps you could follow to crawl this forum:

1. **Initial Page Load**: Load the main page of the forum at `https://www.clien.net/service/board/park`. This page will have a list of current topics.

2. **Extract Post Details**: For each post listed on the page, extract details such as the post's title, URL, author, and date posted. These details are usually found within HTML elements like `<a>` tags for links and `<span>` or `<div>` tags for text.

3. **Pagination**: The forum will have multiple pages. You need to handle pagination by either finding the link to the next page and loading it or by incrementally changing the URL if the site uses a predictable URL structure for pages.

4. **Visit Individual Posts (Optional)**: If you need more detailed information from each post (like the full text or comments), you would then need to visit each post's URL and scrape the information from there.

5. **Respect the Site’s Rules**: Make sure to check the website's `robots.txt` file and terms of service to ensure you're allowed to scrape their data and to understand any limitations they might have on how their site can be crawled.

6. **Use a Web Scraping Tool**: You can use web scraping tools or libraries like Beautiful Soup (for Python), Scrapy, or others depending on your programming language preference.

7. **Handle Dynamically Loaded Content**: If the website uses JavaScript to load content dynamically, you might need tools like Selenium to interact with the webpage as a browser would.

Remember, web scraping should always be done with respect to the website’s terms of service and legal considerations regarding data usage.

## 사용자 입력:
> 한글로 설명해

## ChatGPT 응답:
> 클리앙의 "모두의 공원" 포럼을 크롤링하기 위한 방법은 다음과 같습니다:

1. **초기 페이지 로드**: `https://www.clien.net/service/board/park` 주소의 포럼 메인 페이지를 로드합니다. 이 페이지에는 현재의 주제들이 나열되어 있습니다.

2. **게시글 세부 정보 추출**: 페이지에 나열된 각 게시글에서, 게시글의 제목, URL, 작성자, 작성 날짜 등의 정보를 추출합니다. 이 정보들은 대개 HTML 요소인 `<a>` 태그(링크용)와 `<span>`이나 `<div>` 태그(텍스트용) 안에 있습니다.

3. **페이지네이션 처리**: 포럼에는 여러 페이지가 있을 것입니다. 다음 페이지로의 링크를 찾아 로드하거나, 사이트가 페이지에 대해 예측 가능한 URL 구조를 사용하는 경우 URL을 증분적으로 변경하여 페이지네이션을 처리해야 합니다.

4. **개별 게시글 방문 (선택 사항)**: 각 게시글에서 더 자세한 정보(전문 또는 댓글 등)가 필요한 경우, 해당 게시글의 URL로 이동하여 그곳에서 정보를 긁어내야 합니다.

5. **사이트 규칙 준수**: 해당 웹사이트의 `robots.txt` 파일과 이용 약관을 확인하여 데이터를 긁어내도 되는지, 그리고 사이트를 어떻게 크롤링할 수 있는지에 대한 제한 사항이 있는지 확인하세요.

6. **웹 스크래핑 도구 사용**: Beautiful Soup(파이썬용), Scrapy 등 프로그래밍 언어에 따라 다양한 웹 스크래핑 도구나 라이브러리를 사용할 수 있습니다.

7. **동적으로 로드되는 콘텐츠 처리**: 웹사이트가 자바스크립트를 사용해 콘텐츠를 동적으로 로드하는 경우, 브라우저처럼 웹페이지와 상호작용할 수 있는 Selenium과 같은 도구가 필요할 수 있습니다.

웹 스크래핑은 항상 해당 웹사이트의 이용 약관 및 데이터 사용과 관련된 법적 고려 사항을 존중하며 수행되어야 합니다.

## 사용자 입력:
> 크롤링 코드를 파이썬으로 만들어줘

## ChatGPT 응답:
> 웹 크롤링을 위한 파이썬 코드를 예시로 제공해 드리겠습니다. 이 예시에서는 Python의 `requests` 라이브러리를 사용하여 클리앙의 "모두의 공원" 포럼 페이지에서 데이터를 가져오고, `BeautifulSoup`을 사용하여 HTML을 파싱합니다. 또한, 여러 페이지를 넘기면서 데이터를 수집하는 기본적인 로직을 포함시킬 것입니다.

먼저 필요한 라이브러리를 설치해야 합니다:

```bash
pip install requests beautifulsoup4
```

그런 다음 아래의 파이썬 코드를 사용할 수 있습니다:

```python
import requests
from bs4 import BeautifulSoup

def crawl_clien_park(forum_url, pages=1):
    for page in range(pages):
        # 페이지 URL 구성
        url = f"{forum_url}?&po={page}"
        response = requests.get(url)
        
        # HTML 파싱
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 각 게시글 정보 추출
        for post in soup.find_all("div", class_="list_item symph_row"):
            title = post.find("span", class_="subject_fixed").get_text(strip=True)
            link = post.find("a", class_="list_subject")["href"]
            print(f"Title: {title}, Link: {link}")

# "모두의 공원" 포럼의 URL
forum_url = "https://www.clien.net/service/board/park"

# 첫 2페이지의 게시글 정보 출력
crawl_clien_park(forum_url, pages=2)
```

이 코드는 "모두의 공원" 포럼의 첫 2페이지에서 게시글의 제목과 링크를 추출합니다. `pages` 변수를 변경하여 원하는 페이지 수를 설정할 수 있습니다. 이 코드는 기본적인 웹 스크래핑을 위한 것이며, 실제 사이트의 구조나 클래스 이름이 변경될 수 있으니, 이를 확인하고 필요에 따라 코드를 조정해야 합니다.

**중요**: 이 코드는 교육적 목적으로만 제공됩니다. 실제로 사용하기 전에 해당 사이트의 크롤링 정책을 확인하고, 사이트에 부담을 주지 않도록 주의해야 합니다.

## 사용자 입력:
> 변경분만 가져오도록 추가로직을 넣어봐

## ChatGPT 응답:
> 변경된 부분만 가져오는 로직을 추가하려면, 이전에 크롤링한 데이터와 새로 크롤링한 데이터를 비교하는 방식을 사용할 수 있습니다. 이를 위해 각 게시글의 고유 식별자(예: 게시글 번호)를 사용하여 이전에 크롤링한 게시글과 새로 크롤링한 게시글을 비교해야 합니다. 

아래 예시 코드는 게시글의 고유 ID를 기반으로 변경된 부분을 추적하는 방법을 보여줍니다. 이 예시에서는 간단히 게시글의 제목을 기준으로 변경을 추적하고 있으나, 실제로는 게시글의 고유 ID 또는 URL을 사용하는 것이 더 정확합니다.

```python
import requests
from bs4 import BeautifulSoup

def get_posts_data(soup):
    posts_data = {}
    for post in soup.find_all("div", class_="list_item symph_row"):
        title = post.find("span", class_="subject_fixed").get_text(strip=True)
        link = post.find("a", class_="list_subject")["href"]
        posts_data[link] = title
    return posts_data

def crawl_clien_park(forum_url, pages=1, previous_data=None):
    new_data = {}

    for page in range(pages):
        url = f"{forum_url}?&po={page}"
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')

        current_data = get_posts_data(soup)
        
        if previous_data:
            # 이전 데이터와 비교하여 변경된 부분 찾기
            for link, title in current_data.items():
                if link not in previous_data or title != previous_data[link]:
                    print(f"Changed or New Post: {title}, Link: {link}")
        else:
            # 첫 크롤링인 경우, 모든 게시글 출력
            for link, title in current_data.items():
                print(f"Title: {title}, Link: {link}")

        new_data.update(current_data)

    return new_data

# 첫 크롤링
forum_url = "https://www.clien.net/service/board/park"
previous_data = crawl_clien_park(forum_url, pages=1)

# 잠시 후 두 번째 크롤링 실행
# 이때 변경된 부분만 출력
# 예를 들어, 5분 후에 다시 크롤링을 실행한다고 가정
# import time
# time.sleep(300) # 5분 대기
# new_data = crawl_clien_park(forum_url, pages=1, previous_data=previous_data)
```

이 코드는 처음 크롤링할 때는 모든 게시글을 출력하고, 두 번째 크롤링에서는 새롭게 추가되거나 변경된 게시글만을 감지하여 출력합니다. 이 방법은 단순한 예시이며, 실제로는 더 복잡한 로직이 필요할 수 있습니다. 게시글의 URL이나 고유 식별자를 사용하는 것이 좋으며, 크롤링 간격도 적절히 설정하여 사이트에 부담을 주지 않도록 해야 합니다.

